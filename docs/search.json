[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Hon Kiu To (James), a second-year Ph.D. student in Criminology.\nYou can find more information about me on my personal website.\nMy research interests include forensic statistics and spatiotemporal crime trend modeling and forecasting. I am currently working on a project about facial recognition technology. More specifically, the project aims at evaluating the accuracy and fairness of the use of facial recognition technology by police.",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 550 Final Project: Spatiotemporal Crime Cluster Analysis and Forecasting in Los Angeles (LA)",
    "section": "",
    "text": "This is my MUSA-550 final project, exploring the evolution of violent crime and property crime hotspots in Los Angeles over time, and forecasting future volumes of violent crimes and property crimes using Seasonal Autoregressive Integrated Moving Average (SARIMA) models.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "MUSA 550 Final Project: Spatiotemporal Crime Cluster Analysis and Forecasting in Los Angeles (LA)",
    "section": "",
    "text": "This is my MUSA-550 final project, exploring the evolution of violent crime and property crime hotspots in Los Angeles over time, and forecasting future volumes of violent crimes and property crimes using Seasonal Autoregressive Integrated Moving Average (SARIMA) models.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "MUSA 550 Final Project: Spatiotemporal Crime Cluster Analysis and Forecasting in Los Angeles (LA)",
    "section": "Background",
    "text": "Background\nThe analysis of crime hotspots, particularly violent crimes, has important implications for crime control strategies. Violent crimes are especially costly in terms of human and economic impact. Identifying clusters of violent crimes and examining the patterns of these hotspots can help law enforcement agencies and policymakers allocate resources more effectively, maximizing the return on police deployment and resource allocation.\nOn the other hand, crime trend analysis and forecasting present an intriguing challenge in Criminology. Crime forecasting has been met with skepticism due to its inherent inaccuracies, especially when predictions lack uncertainty assessments. Point forecasts, which predict a single outcome, are often uninformative without understanding the range of possible outcomes. However, using a range of forecasts, represented by “forecast cones”, can add nuance to these predictions. These cones provide insight into the uncertainty of forecasts, as well as potential factors that might influence crime rates, helping them to refine the models used for prediction.\nGiven that budgets are always limited, combining hotspot analysis with more precise forecasts can help policymakers and law enforcement agencies optimize their resource allocation. By targeting areas with the highest crime rates and considering the uncertainty in crime forecasts, they can design more effective budget and resource allocation plans, ultimately improving crime control efficacy.\nThis project is divided into 3 parts:\n\n\nExploratory analysis of the crime data set in LA.\nClustering analysis of the crime data set to study the evolution of crime hotspots over time.\nCrime forecasting using SARIMA models.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "analysis/2-exploratory_data_analysis.html",
    "href": "analysis/2-exploratory_data_analysis.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Let’s import all the necessary libraries.\n\n\nCode\n# Basics\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\n\n# Data Visualization-related Libraries\n\n## matplotlib\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport matplotlib.dates as mdates\n\n## seaborn\nimport seaborn as sns\n\n## folium\nimport folium\nfrom folium import plugins\nfrom folium.plugins import DualMap\nfrom ipywidgets import interact\nimport panel as pn\n\npn.extension()\n\n%matplotlib inline\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nWe begin with a simple time series plot for the trends of violent crimes and property crimes across the years.\n\n\nTrends of Violent and Property Crimes over time (From Jan 2010 to Dec 2024)\nLoad in the pre-processed “LAcrime” data frame and the shapefile for all LAPD reporting districts.\n\n\nCode\nLAcrime = pd.read_parquet(\"LAcrime_trimmed\")\nLAPD_rd = gpd.read_file(\"./LAPD_Reporting_Districts_7103070480637650964/LAPD_Reporting_Districts.shp\")\n\n\nSubset “LAcrime” into two objects: (i) “LAcrime_violent”, containing all the violent crimes, and (ii) “LAcrime_property”, containing all the property crimes.\n\n\nCode\nLAcrime_violent = LAcrime[LAcrime[\"Crime Type\"] == \"violent\"].copy()\nLAcrime_property = LAcrime[LAcrime[\"Crime Type\"] == \"property\"].copy()\n\n\nNow, group the number of violent crimes and property crimes by month.\n\n\nCode\n# Group the total number of violent and property crimes by month.\nviolent_crimes_per_month = LAcrime_violent.groupby(LAcrime_violent[\"Date\"].dt.to_period(\"M\")).size().reset_index(name = \"Count\")\nviolent_crimes_per_month\n\nproperty_crimes_per_month = LAcrime_property.groupby(LAcrime_property[\"Date\"].dt.to_period(\"M\")).size().reset_index(name = \"Count\")\nproperty_crimes_per_month\n\n# Convert the \"Date\" column from YYYY-MM format to datetime.\nviolent_crimes_per_month[\"Date\"] = violent_crimes_per_month[\"Date\"].dt.to_timestamp()\nproperty_crimes_per_month[\"Date\"] = property_crimes_per_month[\"Date\"].dt.to_timestamp()\n\n\n\nWe can take a look at the trends of the monthly number of violent and property crimes from January 2010 to December 2024.\n\n\nCode\nfig, ax = plt.subplots(figsize = (12, 6))\n\n# Plotting violent crimes.\nax.plot(violent_crimes_per_month[\"Date\"], violent_crimes_per_month[\"Count\"], \n         label = \"Violent Crimes\", color = \"#FF6F61\", linestyle = \"-\", marker = \"o\", linewidth = 1)\n\n# Plotting property crimes.\nax.plot(property_crimes_per_month[\"Date\"], property_crimes_per_month[\"Count\"], \n         label = \"Property Crimes\", color = \"#6CA6E0\", linestyle = \"--\", marker = \"o\", linewidth = 1)\n\n# Adding titles and labels.\nax.set_title(\"Trends in Violent vs Property Crimes in Los Angeles (Jan 2010 - Dec 2024)\", fontsize = 14, fontweight = \"bold\")\nax.set_xlabel(\"Date\", fontsize = 12, fontweight = \"bold\")\nax.set_ylabel(\"Number of Crimes\", fontsize=12, fontweight = \"bold\")\n\n# Format the x-axis to display date labels in \"YYYY MMM\" format.\nax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y %b\"))\n\n# Set x-axis limits.\nax.set_xlim(pd.Timestamp(\"2009-07-01\"), pd.Timestamp(\"2025-01-01\"))\n\n# Set the locator to show major ticks every 6 months to avoid overcrowding.\nax.xaxis.set_major_locator(mdates.MonthLocator(interval = 6))\n\n# Rotate x-axis labels by 45 degrees anti-clockwise for better readability.\nax.tick_params(axis = \"x\", rotation = 45)\n\n# Adding grid and legend.\nax.grid(True, which = \"both\", linestyle = \"--\", linewidth = 0.5, alpha = 0.7)\n\n# Add thicker gridlines for each year.\nfor year in range(2010, 2026):\n    ax.axvline(pd.Timestamp(f\"{year}-01-01\"), color = \"gray\", linestyle = \"-\", linewidth = 0.75, alpha = 0.7)\n    \n# Remove the top and right axes lines.\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n\nax.legend(loc = \"best\")\n\n\n\n\n\n\n\n\n\n\nDiscussions:\n\nThere have been more property crimes than violent crimes over the years.\nBoth violent crimes and property crimes exhibit seasonal patterns.\nThere are some suspicions regarding missing or incomplete data. For instance, the number of crimes, both violent and property, in 2010 are so low that seems unrealistic. As we can see later below, it looks like the crime records for a number of precincts are missing. Similar doubts can be casted for years like 2013, 2014.\nThe number of violent crimes and property crimes drop drastically as they approach September 2024. However, this is probably because not all crimes are recorded when the data is downloaded. With the data becoming more complete, I expect both violent crimes and property crimes to return to similar levels as in their past.\n\nThe potential incompleteness of the data poses additional challenge for the forecasting task.\n\n\n\nNext, let’s try to explore the breakdowns of crimes by day of week. This preliminary exploration may shed light to whether there are weekday/weekend effects for certain type(s) of crime.\n\n\n\nBreakdowns of Violent and Property Crimes by Day of Week\n\n\nCode\n# Set the day order for consistent plotting.\nday_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\n# Group the data to get counts of crimes by day and description.\nviolent_crime_counts = LAcrime_violent.groupby([\"Day\", \"Crime_Description\"]).size().reset_index(name = \"Count\")\nproperty_crime_counts = LAcrime_property.groupby([\"Day\", \"Crime_Description\"]).size().reset_index(name = \"Count\")\n\n# Customize the ordering of the labels in the bar charts.\ncustom_labels_violent = [\"Aggravated Assaults\", \"Homicide\", \"Rape\", \"Robbery\", \"Simple Assaults\"]\ncustom_labels_property = [\"Burglary\", \"Motor Vehicle Theft\", \"Other Theft\", \"Personal Theft\", \"Theft from Vehicle\"]\n\n# Customize the color ordering of the labels in the bar charts.\nviolent_colors = [sns.color_palette(\"Reds\", n_colors = 5)[i] for i in [2, 4, 3, 1, 0]]\nproperty_colors = [sns.color_palette(\"Blues\", n_colors = 5)[i] for i in [4, 3, 1, 0, 2]]\n\nfig, ax = plt.subplots(1, 2, figsize = (12, 4), sharey = True)\n\n# Plot violent crimes breakdown.\nsns.barplot(x = \"Day\", y = \"Count\", hue = \"Crime_Description\", data = violent_crime_counts, \n            ax = ax[0], order = day_order, palette = violent_colors, zorder = 999)\nax[0].set_title(\"Violent Crimes by Day of the Week\", fontsize = 14, fontweight = \"bold\")\nax[0].set_xlabel(\"Day of the Week\")\nax[0].set_xticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"])\nax[0].set_ylabel(\"Crime Count\")\nax[0].grid(True, axis = \"y\")\n\n# Remove the top and right axes lines.\nax[0].spines[\"top\"].set_visible(False)\nax[0].spines[\"right\"].set_visible(False)\n\nhandles_violent, _ = ax[0].get_legend_handles_labels()  # Get current handles and labels\nax[0].legend(handles_violent, custom_labels_violent, loc = \"upper center\", bbox_to_anchor = (0.5, -0.15), ncol = 2, \n             title = \"Crime Type\", frameon = False)\n\n# Plot property crimes breakdown.\nsns.barplot(x = \"Day\", y = \"Count\", hue = \"Crime_Description\", data = property_crime_counts, \n            ax = ax[1], order = day_order, palette = property_colors, zorder = 999)\nax[1].set_title(\"Property Crimes by Day of the Week\", fontsize = 14, fontweight = \"bold\")\nax[1].set_xlabel(\"Day of the Week\")\nax[1].set_xticklabels([\"Mon\", \"Tue\", \"Wed\", \"Thurs\", \"Fri\", \"Sat\", \"Sun\"])\nax[1].set_ylabel(\"\")\nax[1].grid(True, axis = \"y\")\n\n# Remove the top and right axes lines.\nax[1].spines[\"top\"].set_visible(False)\nax[1].spines[\"right\"].set_visible(False)\n\nhandles_property, _ = ax[1].get_legend_handles_labels()  # Get current handles and labels\nax[1].legend(handles_property, custom_labels_property, loc = \"upper center\", bbox_to_anchor = (0.5, -0.15), ncol = 2, \n             title = \"Crime Type\", frameon = False)\n\n\n\n\n\n\n\n\n\n\nDiscussions:\n\nGeneral conclusions from both bar charts:\n\nThe volume of different violent crimes and property crimes are at very similar levels regardless of the day of the week, suggesting that there might not be a significant day effect in general.\nHowever, this interpretation warrants caution. The above bar charts aggregate the crime counts over all the years, so we cannot dismiss the possibility that the day effect exists in some years but diminishes once we aggregate over the years.\n\nViolent crimes (Left bar chart):\n\nHomicide, rape, and robbery do not seem to exhibit day effects.\nAssault-type crimes (Aggravated Assaults and Simple Assaults), on the other hand, seem to exhibit some observable day effects. In particular, there are several thousands more assaults during weekends (Saturdays and Sundays).\n\nProperty crimes (Right bar chart):\n\nPersonal theft does not seem to exhibit a day effect.\nThe rest (Burglary, Motor Vehicle Theft, Theft from Vehicle, and Other Theft) all share a similar trend, peaking on Fridays and then dropping on Saturdays and Sundays.\n\n\n\n\nThe time series plot only shows the change in the volume of violent and property crimes over time. It also worths investigating how these incidents vary spatially. For this purpose, we look at the spatial distributions of violent crimes and property crimes over a couple time periods.\n\n\n\nInteractive map of violent and property crimes by time period\nFirst, let’s set up the features for the base map consisting of LAPD reporting districts.\n\n\nCode\nLAPD_rd = LAPD_rd.to_crs(epsg = 4326)\n\n\n\n\nCode\n# Generate 21 unique colors using a colormap.\nnum_precs = 21\ncmap = matplotlib.colormaps.get_cmap(\"tab20b\")\nprec_values = LAPD_rd[\"PREC\"].unique()\nprec_colors = {\n    prec: cmap(i / num_precs) for i, prec in enumerate(sorted(prec_values))\n}\n\n# Convert RGBA to Hex for folium compatibility.\nprec_colors_hex = {\n    prec: f'#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}' for prec, (r, g, b, _) in prec_colors.items()\n}\n\n# Define the style function.\ndef style_function(feature):\n    prec = feature[\"properties\"][\"PREC\"]\n    color = prec_colors_hex.get(prec, \"#808080\")\n    return {\n        \"fillColor\": color,\n        \"color\": \"black\",\n        \"weight\": 1,\n        \"fillOpacity\": 0.6,\n    }\n\n\nTo distinguish between different types of crimes under the umbrella categories “violent” and “property”, each crime type is assigned a specific color.\n\n\nCode\n# Define a color map for Crime_Description.\ncrime_colors = {\n    \"Homicide\": \"#e41a1c\",\n    \"Rape\": \"#377eb8\",\n    \"Aggravated Assaults\": \"#4daf4a\",\n    \"Robbery\": \"#984ea3\",\n    \"Simple Assaults\": \"#ff7f00\",\n    \"Burglary\": \"#a6cee3\",\n    \"Motor Vehicle Theft\": \"#1f78b4\",\n    \"Theft from Vehicle\": \"#b2df8a\",\n    \"Other Theft\": \"#33a02c\",\n    \"Personal Theft\": \"#fb9a99\"\n}\n\ndef get_crime_color(crime_desc):\n    return crime_colors.get(crime_desc, \"gray\")\n\n\nFor each time period, two side-by-side maps are returned. On the left is the spatial distribution of violent crimes, and on the right, the spatial distribution of property crimes.\n\n\nCode\n# Function to update the map based on selected year and month.\ndef crime_scatter_map(year, month):\n    temp = LAcrime[(LAcrime[\"Month\"] == month) & (LAcrime[\"Year\"] == year)]\n    \n    # Separate data for violent and property crimes.\n    violent_crimes = temp[temp[\"Crime Type\"] == \"violent\"]\n    property_crimes = temp[temp[\"Crime Type\"] == \"property\"]\n    \n    # Initialize DualMap: one for violent crimes (left) and one for property crimes (right).\n    m = DualMap(location = [34.0522, -118.2437], zoom_start = 9)\n    \n    # Add LAPD reporting districts.\n    folium.GeoJson(\n        LAPD_rd,\n        style_function = style_function,\n        tooltip = folium.GeoJsonTooltip(fields = [\"PREC\"], aliases = [\"Precinct:\"], localize = True),\n        name = \"LAPD Reporting Districts\"\n    ).add_to(m.m1)\n    \n    folium.GeoJson(\n        LAPD_rd,\n        style_function = style_function,\n        tooltip = folium.GeoJsonTooltip(fields = [\"PREC\"], aliases = [\"Precinct:\"], localize = True),\n        name = \"LAPD Reporting Districts\"\n    ).add_to(m.m2)\n    \n    # Add violent crimes to the left map.\n    for _, row in violent_crimes.iterrows():\n        crime_desc = row[\"Crime_Description\"]\n        color = get_crime_color(crime_desc)\n        \n        folium.CircleMarker(\n            [row[\"LAT\"], row[\"LON\"]],\n            radius = 2,\n            color = color,\n            fill = True,\n            fill_color = color,\n            tooltip = f'{crime_desc}'\n        ).add_to(m.m1)\n    \n    # Add property crimes to the right map.\n    for _, row in property_crimes.iterrows():\n        crime_desc = row[\"Crime_Description\"]\n        color = get_crime_color(crime_desc)\n        \n        folium.CircleMarker(\n            [row[\"LAT\"], row[\"LON\"]],\n            radius = 2,\n            color = color,\n            fill = True,\n            fill_color = color,\n            tooltip = f'{crime_desc}'\n        ).add_to(m.m2)\n    \n    # Return the map.\n    return pn.panel(m, height = 500, width = 800)\n\n\nWe suspect there might be missingness in the data for 2010. Let’s take a look at the maps for, e.g., January 2010.\n\ncrime_scatter_map(2010, 1)\n\n\n\n\n\n  \n\n\n\n\nIt looks like the incident records for more than 10 precincts are missing! Now, let’s look at some other months and years.\n\ncrime_scatter_map(2017, 1)\n\n\n\n\n\n  \n\n\n\n\nThis period seem to exhibit a certain degree of missingness as well. How about 2018?\n\ncrime_scatter_map(2018, 1)\n\n\n\n\n\n  \n\n\n\n\nJudging by the map in January 2018 onwards, and combine the observations with the time series plot at the beginning, it seems to be safer to utilize data from 2018 onwards for further analysis. This approach could help mitigate the potential missing data problem. Besides, using more recent data could be a natural option as well, given the relevance of more recent data compared to more historical data in terms of informing and inferring potential dynamics in the future.\n\nRemark:\nBelow is an attempt to create a panel dashboard with year and month sliders for more interactive display. While it can be executed normally in Jupyter notebook, it needs to be hosted on dynamic servers. This is part of the future work.\n\n\nCode\n# Create the sliders for year and month\nyear_selector = pn.widgets.IntSlider(name = \"Year\", start = 2010, end = 2024, value = 2019)\nmonth_selector = pn.widgets.IntSlider(name = \"Month\", start = 1, end = 12, value = 1)\n\n# Bind the function to the widgets\ninteractive_crime_scatter_map = pn.bind(crime_scatter_map, year = year_selector, month = month_selector)\n\n# Layout the widgets and map together\npn.Column(year_selector, month_selector, interactive_crime_scatter_map)",
    "crumbs": [
      "Analysis",
      "Data Visualization"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/4-crime_trend_forecasting.html",
    "href": "analysis/4-crime_trend_forecasting.html",
    "title": "Crime Trend Forecasting with SARIMA Models",
    "section": "",
    "text": "Let’s import all the necessary libraries.\n\n\nCode\n# Basics\nimport numpy as np\nimport pandas as pd\nfrom pmdarima import auto_arima\n\n# Data Visualization-related Libraries\n\n## altair\nimport altair as alt\n\n## Panel\nimport panel as pn\n\npn.extension()\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nLoad in the pre-processed “LAcrime” data frame.\n\n\nCode\nLAcrime = pd.read_parquet(\"LAcrime_trimmed\")\n\n\nNext, we group the number of crimes by area, year, month, and crime type.\n\ncrime_counts = LAcrime.groupby([\"AREA\", \"Year\", \"Month\", \"Crime Type\"]).agg({\n    \"Crm Cd\": \"count\"\n})\n\nSince there are some missingness/incompleteness in the data, we create a multiindex of all combinations of area, year, month, and crime type, and then fill in zeros for all the missing combinations.\n\nall_combinations = pd.MultiIndex.from_product(\n    [LAcrime[\"AREA\"].unique(),\n     LAcrime[\"Year\"].unique(), \n     LAcrime[\"Month\"].unique(),  \n     LAcrime[\"Crime Type\"].unique()],\n    names = [\"AREA\", \"Year\", \"Month\", \"Crime Type\"]\n)\n\n\ncrime_counts = crime_counts.reindex(all_combinations, fill_value = 0)\ncrime_counts = crime_counts.reset_index()\n\n\ncrime_counts = crime_counts.sort_values(by = [\"AREA\", 'Year', 'Month']).reset_index(drop = True)\n\nThe resulting “crime_counts” object looks like this:\n\ncrime_counts.head()\n\n\n\n\n\n\n\n\nAREA\nYear\nMonth\nCrime Type\nCrm Cd\n\n\n\n\n0\n1\n2010\n1\nviolent\n224\n\n\n1\n1\n2010\n1\nproperty\n223\n\n\n2\n1\n2010\n2\nviolent\n182\n\n\n3\n1\n2010\n2\nproperty\n166\n\n\n4\n1\n2010\n3\nviolent\n245\n\n\n\n\n\n\n\n\nSARIMA forecasting\n\nWhy SARIMA?\nCrime exhibits seasonality. Also, ARIMA model is a popular tool for forecasting future crimes. As such, SARIMA model is the natural extension which incorporates seasonality.\nBelow, we model and forecast the number of violent and property crimes in the future. - The training data consists of data from January 2018 up to December 2022. Reasons for choosing 2018 as the starting year: - As explored in previous sections, there could be potential missingness or incomplete records in prior years. Including them into the training data might introduce additional noise due to missingness or incompleteness. - Data from more recent years should be more relevant for predicting future crimes. - We are withholding 2023 data as the validation set, built upon the assumption that 2023 data should be more complete and reliable. - The aim is the forecast the number of violent and property crimes in 2024.\n\n\nCode\n# Perform SARIMA forecasting over each crime type and each area.\ncrime_types = [\"violent\", \"property\"]\nareas = crime_counts[\"AREA\"].unique()\n\nforecast_results = []\n\nfor crime_type in crime_types:\n    for area in areas:\n        area_data = crime_counts[(crime_counts[\"AREA\"] == area) & (crime_counts[\"Crime Type\"] == crime_type)].copy()\n        \n        # Create a Date column.\n        area_data[\"Date\"] = pd.to_datetime(area_data[[\"Year\", \"Month\"]].assign(DAY=1))\n        area_data.set_index(\"Date\", inplace=True)\n\n        # Training data: All data from the beginning of January 2018 until the end of December 2022.\n        train_data = area_data[(area_data.index &gt;= \"2018-01-01\") & (area_data.index &lt;= \"2022-12-31\")].copy()\n        \n        # Log transformation.\n        train_data[\"Log_Crime_Count\"] = np.log1p(train_data[\"Crm Cd\"])\n\n        # Fit SARIMA models.\n        target_var = \"Crm Cd\"\n        target_var_log = \"Log_Crime_Count\"\n        \n        y_train = train_data[target_var]\n        y_train_log = train_data[target_var_log]\n\n        # Fit on both versions.\n        model = auto_arima(y_train, seasonal = True, m = 12, stepwise = True, trace = False)\n        model_log = auto_arima(y_train_log, seasonal = True, m = 12, stepwise = True, trace = False)\n\n        # Forecast for the next 24 months.\n        forecast, conf_int = model.predict(n_periods = 24, return_conf_int = True)\n        forecast_log, conf_int_log = model_log.predict(n_periods = 24, return_conf_int = True)\n\n        # Back-transform log forecasts to original scale.\n        forecast_original = forecast\n        conf_int_original = conf_int\n        forecast_log_original = np.expm1(forecast_log)\n        conf_int_log_original = np.expm1(conf_int_log)\n\n        # Clip the lower bound of the original prediction intervals to 0.\n        conf_int_clipped = conf_int_original.copy()\n        conf_int_clipped[:, 0] = np.maximum(conf_int_clipped[:, 0], 0)\n        \n        # Extract SARIMA orders (p, d, q) and seasonal orders (P, D, Q, m).\n        order = model.order  # (p, d, q)\n        seasonal_order = model.seasonal_order  # (P, D, Q, m)\n        \n        order_log = model_log.order  # (p, d, q)\n        seasonal_order_log = model_log.seasonal_order  # (P, D, Q, m)\n\n        # Prepare forecast data for both versions.\n        forecast_df = pd.DataFrame({\n            \"Crime_Type\": [crime_type] * 24,\n            \"AREA\": [area] * 24,\n            \"Forecast_Original\": forecast_original,\n            \"Lower_95_Original\": conf_int_original[:, 0],\n            \"Upper_95_Original\": conf_int_original[:, 1],\n            \"Lower_95_Clipped\": conf_int_clipped[:, 0],\n            \"Upper_95_Clipped\": conf_int_clipped[:, 1],\n            \"SARIMA_Order\": [order] * 24,\n            \"Seasonal_Order\": [seasonal_order] * 24,\n            \"Forecast_Log_Transformed\": forecast_log_original,\n            \"Lower_95_Log_Transformed\": conf_int_log_original[:, 0],\n            \"Upper_95_Log_Transformed\": conf_int_log_original[:, 1],\n            \"SARIMA_Order_Log_Transformed\": [order_log] * 24,\n            \"Seasonal_Order_Log_Transformed\": [seasonal_order_log] * 24\n        })\n\n        # Add Year and Month.\n        start_date = \"2023-01-01\"\n        forecast_dates = pd.date_range(start = start_date, periods = 24, freq = 'MS')\n        forecast_df[\"Year\"] = forecast_dates.year\n        forecast_df[\"Month\"] = forecast_dates.month\n\n        forecast_results.append(forecast_df)\n\n# Combine all forecasts into one DataFrame.\nfinal_forecast_df = pd.concat(forecast_results, ignore_index = True)\n\n\nFor each area, we plot the time series for violent crimes and property crimes, including the training data, validation data, the forecasts, and the prediction intervals.\n\n\nCode\ndate_range = pd.date_range(start = \"2018-01-01\", periods = 84, freq = 'MS')\nforecast_col = \"Forecast\"\nlower_col = \"Lower_95\"\nupper_col = \"Upper_95\"\n\ndef forecast_plot(area):\n    # Define the combined data frame for violent crimes.\n    temp_forecast_violent = final_forecast_df[(final_forecast_df[\"Crime_Type\"] == \"violent\") & (final_forecast_df[\"AREA\"] == area)]\n    temp_violent = crime_counts[(crime_counts[\"Year\"] &gt;= 2018) & (crime_counts[\"Crime Type\"] == \"violent\") & (crime_counts[\"AREA\"] == area)]\n    combined_df_violent = pd.DataFrame({\n            \"Year\": date_range.year,\n            \"Month\": date_range.month,\n            \"Crime_Type\": [\"violent\"] * 84,\n            \"AREA\": [area] * 84,\n            \"Count\": pd.concat([temp_violent[\"Crm Cd\"][:-12], pd.Series([np.nan] * 12)], ignore_index = True),\n            \"Forecast\": pd.concat([temp_violent[\"Crm Cd\"][:-24], temp_forecast_violent[\"Forecast_Original\"]], ignore_index = True),\n            \"Lower_95\": pd.concat([temp_violent[\"Crm Cd\"][:-24], temp_forecast_violent[\"Lower_95_Original\"]], ignore_index = True),\n            \"Upper_95\": pd.concat([temp_violent[\"Crm Cd\"][:-24], temp_forecast_violent[\"Upper_95_Original\"]], ignore_index = True)\n        })\n    combined_df_violent[\"Date\"] = pd.to_datetime(combined_df_violent[[\"Year\", \"Month\"]].assign(Day = 1))\n    \n    # Define the combined data frame for property crimes.\n    temp_forecast_property = final_forecast_df[(final_forecast_df[\"Crime_Type\"] == \"property\") & (final_forecast_df[\"AREA\"] == area)]\n    temp_property = crime_counts[(crime_counts[\"Year\"] &gt;= 2018) & (crime_counts[\"Crime Type\"] == \"property\") & (crime_counts[\"AREA\"] == area)]\n    combined_df_property = pd.DataFrame({\n            \"Year\": date_range.year,\n            \"Month\": date_range.month,\n            \"Crime_Type\": [\"property\"] * 84,\n            \"AREA\": [area] * 84,\n            \"Count\": pd.concat([temp_property[\"Crm Cd\"][:-12], pd.Series([np.nan] * 12)], ignore_index = True),\n            \"Forecast\": pd.concat([temp_property[\"Crm Cd\"][:-24], temp_forecast_property[\"Forecast_Original\"]], ignore_index = True),\n            \"Lower_95\": pd.concat([temp_property[\"Crm Cd\"][:-24], temp_forecast_property[\"Lower_95_Original\"]], ignore_index = True),\n            \"Upper_95\": pd.concat([temp_property[\"Crm Cd\"][:-24], temp_forecast_property[\"Upper_95_Original\"]], ignore_index = True)\n        })\n    combined_df_property[\"Date\"] = pd.to_datetime(combined_df_property[[\"Year\", \"Month\"]].assign(Day = 1))\n    \n    # Brush selection for highlighting a period.\n    brush = alt.selection_interval(encodings = ['x'])\n    \n    # Base chart for both violent and property crime.\n    base_violent = alt.Chart(combined_df_violent).encode(\n        x = alt.X(\"Date:T\", title = \"Time Period\", axis = alt.Axis(format = \"%b %Y\"))\n    )\n\n    base_property = alt.Chart(combined_df_property).encode(\n        x = alt.X(\"Date:T\", title = \"Time Period\", axis = alt.Axis(format = \"%b %Y\"))\n    )\n\n    # Historical line and tooltip.\n    historical_line_violent_joint = base_violent.mark_line(color = \"blue\").transform_filter(\n        (alt.datum.Year &lt;= 2022) | ((alt.datum.Year == 2023) & (alt.datum.Month == 1))\n    ).encode(\n        y = alt.Y(f'{forecast_col}:Q', title = \"Crime Count\"),\n        tooltip = [\n            alt.Tooltip(\"Year:O\", title = \"Year\"),\n            alt.Tooltip(\"Month:O\", title = \"Month\"),\n            alt.Tooltip(f'{forecast_col}:Q', title = \"Crime Count\")\n        ]\n    )\n    \n    historical_line_property_joint = base_property.mark_line(color = \"blue\").transform_filter(\n        (alt.datum.Year &lt;= 2022) | ((alt.datum.Year == 2023) & (alt.datum.Month == 1))\n    ).encode(\n        y = alt.Y(f'{forecast_col}:Q', title = \"Crime Count\"),\n        tooltip = [\n            alt.Tooltip(\"Year:O\", title = \"Year\"),\n            alt.Tooltip(\"Month:O\", title = \"Month\"),\n            alt.Tooltip(f'{forecast_col}:Q', title = \"Crime Count\")\n        ]\n    )\n\n    historical_line_violent = base_violent.mark_line(color = \"blue\").transform_filter(\n        alt.datum.Year &lt;= 2023\n    ).encode(\n        y = alt.Y(\"Count:Q\", title = \"Crime Count\"),\n        tooltip = [\n            alt.Tooltip(\"Year:O\", title = \"Year\"),\n            alt.Tooltip(\"Month:O\", title = \"Month\"),\n            alt.Tooltip(\"Count:Q\", title = \"Crime Count\")\n        ]\n    )\n    \n    historical_line_property = base_property.mark_line(color = \"blue\").transform_filter(\n        alt.datum.Year &lt;= 2023\n    ).encode(\n        y = alt.Y(\"Count:Q\", title = \"Crime Count\"),\n        tooltip = [\n            alt.Tooltip(\"Year:O\", title = \"Year\"),\n            alt.Tooltip(\"Month:O\", title = \"Month\"),\n            alt.Tooltip(\"Count:Q\", title = \"Crime Count\")\n        ]\n    )\n\n    # Forecast line and tooltip.\n    forecast_line_violent = base_violent.mark_line(color = \"red\").transform_filter(\n        alt.datum.Year &gt;= 2023\n    ).encode(\n        y = alt.Y(f'{forecast_col}:Q'),\n        tooltip = [\n            alt.Tooltip(\"Year:O\", title = \"Year\"),\n            alt.Tooltip(\"Month:O\", title = \"Month\"),\n            alt.Tooltip(f'{forecast_col}:Q', title = \"Forecast\"),\n            alt.Tooltip(f'{lower_col}:Q', title = \"Lower Bound (95%)\"),\n            alt.Tooltip(f'{upper_col}:Q', title = \"Upper Bound (95%)\")\n        ]\n    )\n    \n    forecast_line_property = base_property.mark_line(color = \"red\").transform_filter(\n        alt.datum.Year &gt;= 2023\n    ).encode(\n        y = alt.Y(f'{forecast_col}:Q'),\n        tooltip = [\n            alt.Tooltip(\"Year:O\", title = \"Year\"),\n            alt.Tooltip(\"Month:O\", title = \"Month\"),\n            alt.Tooltip(f'{forecast_col}:Q', title = \"Forecast\"),\n            alt.Tooltip(f'{lower_col}:Q', title = \"Lower Bound (95%)\"),\n            alt.Tooltip(f'{upper_col}:Q', title = \"Upper Bound (95%)\")\n        ]\n    )\n\n    # Markers for historical and forecast data.\n    historical_markers_violent_joint = base_violent.mark_point(color = \"blue\").transform_filter(\n        alt.datum.Year &lt;= 2022\n    ).encode(\n        y = alt.Y(f'{forecast_col}:Q'),\n        tooltip = [\n            alt.Tooltip(\"Year:O\", title = \"Year\"),\n            alt.Tooltip('Month:O', title = \"Month\"),\n            alt.Tooltip(f'{forecast_col}:Q', title = \"Crime Count\")\n        ]\n    )\n    \n    historical_markers_property_joint = base_property.mark_point(color = \"blue\").transform_filter(\n        alt.datum.Year &lt;= 2022\n    ).encode(\n        y = alt.Y(f'{forecast_col}:Q'),\n        tooltip = [\n            alt.Tooltip(\"Year:O\", title = \"Year\"),\n            alt.Tooltip(\"Month:O\", title = \"Month\"),\n            alt.Tooltip(f'{forecast_col}:Q', title = \"Crime Count\")\n        ]\n    )\n\n    historical_markers_violent = base_violent.mark_point(color = \"blue\").transform_filter(\n        alt.datum.Year &lt;= 2023\n    ).encode(\n        y = alt.Y(\"Count:Q\"),\n        tooltip = [\n            alt.Tooltip(\"Year:O\", title = \"Year\"),\n            alt.Tooltip(\"Month:O\", title = \"Month\"),\n            alt.Tooltip(\"Count:Q\", title = \"Crime Count\")\n        ]\n    )\n    \n    historical_markers_property = base_property.mark_point(color = \"blue\").transform_filter(\n        alt.datum.Year &lt;= 2023\n    ).encode(\n        y = alt.Y(\"Count:Q\"),\n        tooltip = [\n            alt.Tooltip(\"Year:O\", title = \"Year\"),\n            alt.Tooltip(\"Month:O\", title = \"Month\"),\n            alt.Tooltip(\"Count:Q\", title = \"Crime Count\")\n        ]\n    )\n\n    forecast_markers_violent = base_violent.mark_point(color = \"red\").transform_filter(\n        alt.datum.Year &gt;= 2023\n    ).encode(\n        y = alt.Y(f'{forecast_col}:Q'),\n        tooltip = [\n            alt.Tooltip(\"Year:O\", title = \"Year\"),\n            alt.Tooltip(\"Month:O\", title = \"Month\"),\n            alt.Tooltip(f'{forecast_col}:Q', title = \"Forecast\"),\n            alt.Tooltip(f'{lower_col}:Q', title = \"Lower Bound (95%)\"),\n            alt.Tooltip(f'{upper_col}:Q', title = \"Upper Bound (95%)\")\n        ]\n    )\n    \n    forecast_markers_property = base_property.mark_point(color = \"red\").transform_filter(\n        alt.datum.Year &gt;= 2023\n    ).encode(\n        y = alt.Y(f'{forecast_col}:Q'),\n        tooltip = [\n            alt.Tooltip(\"Year:O\", title = \"Year\"),\n            alt.Tooltip(\"Month:O\", title = \"Month\"),\n            alt.Tooltip(f'{forecast_col}:Q', title = \"Forecast\"),\n            alt.Tooltip(f'{lower_col}:Q', title = \"Lower Bound (95%)\"),\n            alt.Tooltip(f'{upper_col}:Q', title = \"Upper Bound (95%)\")\n        ]\n    )\n\n    # Confidence interval shading\n    confidence_area_violent = base_violent.mark_area(opacity = 0.2, color = \"red\").transform_filter(\n        alt.datum.Year &gt;= 2023\n    ).encode(\n        y = alt.Y(f'{lower_col}:Q'),\n        y2 = upper_col\n    )\n    \n    confidence_area_property = base_property.mark_area(opacity = 0.2, color = \"red\").transform_filter(\n        alt.datum.Year &gt;= 2023\n    ).encode(\n        y = alt.Y(f'{lower_col}:Q'),\n        y2 = upper_col\n    )\n\n    # Combine layers for violent and property crimes\n    violent_chart = (\n        historical_line_violent +\n        historical_line_violent_joint +\n        forecast_line_violent +\n        historical_markers_violent +\n        historical_markers_violent_joint +\n        forecast_markers_violent +\n        confidence_area_violent\n    ).properties(\n        title = \"Violent Crime Forecast\",\n        width = 700,\n        height = 300\n    ).add_selection(brush)\n    \n    property_chart = (\n        historical_line_property +\n        historical_line_property_joint +\n        forecast_line_property +\n        historical_markers_property +\n        historical_markers_property_joint +\n        forecast_markers_property +\n        confidence_area_property\n    ).properties(\n        title = \"Property Crime Forecast\",\n        width = 700,\n        height = 300\n    ).add_selection(brush)\n    \n    # Combine both charts side by side.\n    final_chart = alt.vconcat(violent_chart, property_chart)\n    \n    return final_chart\n\n    # return pn.panel(final_chart)\n\n\nLet’s have a look at the forecasts for Area 21.\n\nforecast_plot(21)\n\n\n\n\n\n\n\nDiscussions:\n\nFor violent crimes, it seems that the forecasts aligned with the actual observations reasonably in the validation period (January 2023 - December 2023).\nOn the other hand, the violent crime forecasts for 2024 are very flat, indicating the forecasts for 2024 are pretty uninformative.\nFor property crimes, the forecasts fluctuate around the same level, but the actual observations are gradually increasing with some indications of exhibiting trends.\n\nOn top of that, we observe that (eventually) stable forecasts are the expectation rather than exception, although sometimes the prediction intervals remain informative. We also acknowledge that even with almost constant forecasts (and sometimes prediction intervals as well), most of the observations lie within the region enclosed by the prediction intervals.\nThere could be multiple reasons behind these observations. - We may not be using sufficiently complicated models. - We may not have enough data to capture persistent long-term trend and/or seasonal effects. - No exogenous variables are included. - SARIMA may not be a good model to start with.",
    "crumbs": [
      "Analysis",
      "Crime Trend Forecasting with SARIMA Models"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "analysis/1-data_preprocessing.html",
    "href": "analysis/1-data_preprocessing.html",
    "title": "Data Pre-processing",
    "section": "",
    "text": "Let’s import all the necessary libraries.\n\n\nCode\n# Basics\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom zipfile import ZipFile # For opening .csv files inside the .zip files.\nfrom itertools import chain # For flattening property and violent crime codes from the nested dictionary.\nimport os\nfrom shapely.geometry import Point\n\n\n\nNow, we can begin prepare the data. The first step is recovering the original data frames from the split files. The original .csv files are too large, so they are pre-split to facilitate uploading. The following codes are used to combine them back to their original forms.\n\ndef load_split_files(directory):\n    all_parts = []\n    for filename in sorted(os.listdir(directory)):\n        if filename.endswith('.csv'):\n            part_df = pd.read_csv(os.path.join(directory, filename))\n            all_parts.append(part_df)\n    return pd.concat(all_parts, ignore_index = True)\n\n# Load the files\ndf_2010_2019 = load_split_files('./data/split_files_2010_2019')\ndf_2020_present = load_split_files('./data/split_files_2020_present')\n\nNote: Both data sets contain more than 1 million rows.\n\nprint(f\"The crime data from 2010 to 2019 contains {len(df_2010_2019)} rows.\")\nprint(f\"The crime data from 2010 to 2019 contains {len(df_2020_present)} rows.\")\n\nThe crime data from 2010 to 2019 contains 1688586 rows.\nThe crime data from 2010 to 2019 contains 1001112 rows.\n\n\nThere is a slight caveat: For the crime data from 2010 to 2019, the variable name for the area code is “AREA” (note the trailing space), but for the crime data starting from 2020, the variable name for the area code is “AREA”. So, I rename the column “AREA” to “AREA” in “df_2010_2019” so that both data frames are consistent in terms of variable names.\nThere are also repeated records, and these are dropped before merging.\n\ndf_2010_2019.rename(columns = {\"AREA \": \"AREA\"}, inplace=True)\ndf_2010_2019 = df_2010_2019.drop_duplicates(subset = [\"DR_NO\"], keep = \"first\")\n\n\n\nNow, I merge the two data frames together.\n\n\nCode\ndata = pd.concat([df_2010_2019, df_2020_present]).reset_index(drop = True)\n\n\nThe descriptions of some of the variables in the data set are provided below.\n\n\nVariables Description\n\nDATE OCC: Presumably date occured. In MM/DD/YYYY format.\nAREA: The LAPD has 21 Community Police Stations referred to as Geographic Areas within the department. These Geographic Areas are sequentially numbered from 1-21.\nCrm Cd: Indicates the crime committed. (Same as Crime Code 1)\nCrm Cd Desc: Defines the Crime Code provided.\nLAT: Latitude\nLON: Longtitude\n\nThese will be the variables to be used in the remaining of the analyses.\n\n# Extract the relevant columns.\n\nLAcrime = data[[\"DATE OCC\", \"AREA\", \"Crm Cd\", \"Crm Cd Desc\", \"LAT\", \"LON\"]].copy()\nLAcrime.head()\n\n\n\n\n\n\n\n\nDATE OCC\nAREA\nCrm Cd\nCrm Cd Desc\nLAT\nLON\n\n\n\n\n0\n02/20/2010 12:00:00 AM\n13\n900\nVIOLATION OF COURT ORDER\n33.9825\n-118.2695\n\n\n1\n09/12/2010 12:00:00 AM\n14\n740\nVANDALISM - FELONY ($400 & OVER, ALL CHURCH VA...\n33.9599\n-118.3962\n\n\n2\n08/09/2010 12:00:00 AM\n13\n946\nOTHER MISCELLANEOUS CRIME\n34.0224\n-118.2524\n\n\n3\n01/05/2010 12:00:00 AM\n6\n900\nVIOLATION OF COURT ORDER\n34.1016\n-118.3295\n\n\n4\n01/02/2010 12:00:00 AM\n1\n122\nRAPE, ATTEMPTED\n34.0387\n-118.2488\n\n\n\n\n\n\n\nCreate a new “Date” column based on “DATE OCC”, and then extract the year, month, and day, and store them in respective columns.\n\n# Convert the dates stored in \"DATE OCC\" into a proper datetime format and store it as a new variable \"Date\".\n\nLAcrime[\"Date\"] = pd.to_datetime(LAcrime[\"DATE OCC\"])\nLAcrime[\"Year\"] = LAcrime[\"Date\"].dt.year\nLAcrime[\"Month\"] = LAcrime[\"Date\"].dt.month\nLAcrime[\"Day\"] = LAcrime[\"Date\"].dt.day_name()\n\nLAcrime.head()\n\n\n\n\n\n\n\n\nDATE OCC\nAREA\nCrm Cd\nCrm Cd Desc\nLAT\nLON\nDate\nYear\nMonth\nDay\n\n\n\n\n0\n02/20/2010 12:00:00 AM\n13\n900\nVIOLATION OF COURT ORDER\n33.9825\n-118.2695\n2010-02-20\n2010\n2\nSaturday\n\n\n1\n09/12/2010 12:00:00 AM\n14\n740\nVANDALISM - FELONY ($400 & OVER, ALL CHURCH VA...\n33.9599\n-118.3962\n2010-09-12\n2010\n9\nSunday\n\n\n2\n08/09/2010 12:00:00 AM\n13\n946\nOTHER MISCELLANEOUS CRIME\n34.0224\n-118.2524\n2010-08-09\n2010\n8\nMonday\n\n\n3\n01/05/2010 12:00:00 AM\n6\n900\nVIOLATION OF COURT ORDER\n34.1016\n-118.3295\n2010-01-05\n2010\n1\nTuesday\n\n\n4\n01/02/2010 12:00:00 AM\n1\n122\nRAPE, ATTEMPTED\n34.0387\n-118.2488\n2010-01-02\n2010\n1\nSaturday\n\n\n\n\n\n\n\n\n\nWe focus on violent crimes and property crimes. The crime codes and categories are based on UCR Reporting.\n\n# Nested dictionary of crime codes.\n\ncrime_codes = {\n    \"Violent\": {\n        \"Homicide\": [110, 113],\n        \"Rape\": [121, 122, 815, 820, 821],\n        \"Robbery\": [210, 220],\n        \"Aggravated Assaults\": [230, 231, 235, 236, 250, 251, 761, 926],\n        \"Simple Assaults\": [435, 436, 437, 622, 623, 624, 625, 626, 627, 647, 763, 928, 930]\n    },\n    \"Property\": {\n        \"Burglary\": [310, 320],\n        \"Motor Vehicle Theft\": [510, 520, 433],\n        \"Theft from Vehicle\": [330, 331, 410, 420, 421],\n        \"Personal Theft\": [350, 351, 352, 353, 450, 451, 452, 453],\n        \"Other Theft\": [341, 343, 345, 440, 441, 442, 443, 444, 445, 470, 471, 472, 473, 474, 475, 480, 485, 487, 491]\n    }\n}\n\n\n# Flatten the violent and property crime codes.\nviolent_codes = list(chain(*crime_codes[\"Violent\"].values()))\nproperty_codes = list(chain(*crime_codes[\"Property\"].values()))\n\nBased on the violent and property crime codes, create two new columns to store the corresponding crime categories (“Crime_Description”) and crime type (“Crime Type”).\n\n# Flatten the crime_codes dictionary to map codes to descriptions.\ncode_to_description = {}\nfor crime_type, subdict in crime_codes.items():\n    for description, codes in subdict.items():\n        for code in codes:\n            code_to_description[code] = description\n\n# Add a \"Crime_Description\" column.\nLAcrime[\"Crime_Description\"] = LAcrime[\"Crm Cd\"].map(code_to_description)\n\n\nLAcrime = LAcrime[LAcrime[\"Crm Cd\"].isin(violent_codes + property_codes)]\n\n# Create conditions for assigning crime types.\nconditions = [\n    LAcrime[\"Crm Cd\"].isin(violent_codes),\n    LAcrime[\"Crm Cd\"].isin(property_codes)\n]\n\n# Define corresponding choices (the crime types).\nchoices = [\"violent\", \"property\"]\n\n# Assign the crime type based on the conditions.\nLAcrime[\"Crime Type\"] = np.select(conditions, choices, default = \"other\")\n\nThe pre-processed data frame looks like this:\n\nLAcrime.head()\n\n\n\n\n\n\n\n\nDATE OCC\nAREA\nCrm Cd\nCrm Cd Desc\nLAT\nLON\nDate\nYear\nMonth\nDay\nCrime_Description\nCrime Type\n\n\n\n\n4\n01/02/2010 12:00:00 AM\n1\n122\nRAPE, ATTEMPTED\n34.0387\n-118.2488\n2010-01-02\n2010\n1\nSaturday\nRape\nviolent\n\n\n5\n01/04/2010 12:00:00 AM\n1\n442\nSHOPLIFTING - PETTY THEFT ($950 & UNDER)\n34.0480\n-118.2577\n2010-01-04\n2010\n1\nMonday\nOther Theft\nproperty\n\n\n6\n01/07/2010 12:00:00 AM\n1\n330\nBURGLARY FROM VEHICLE\n34.0389\n-118.2643\n2010-01-07\n2010\n1\nThursday\nTheft from Vehicle\nproperty\n\n\n7\n01/08/2010 12:00:00 AM\n1\n230\nASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT\n34.0435\n-118.2427\n2010-01-08\n2010\n1\nFriday\nAggravated Assaults\nviolent\n\n\n8\n01/09/2010 12:00:00 AM\n1\n230\nASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT\n34.0450\n-118.2640\n2010-01-09\n2010\n1\nSaturday\nAggravated Assaults\nviolent\n\n\n\n\n\n\n\n\n\nExport the “LAcrime” dataframe as a compressed Python object. This will be used in further analyses.\n\nLAcrime.to_parquet(\"LAcrime_trimmed\", compression = \"snappy\")",
    "crumbs": [
      "Analysis",
      "Data Pre-processing"
    ]
  },
  {
    "objectID": "analysis/3-crime_clustering.html",
    "href": "analysis/3-crime_clustering.html",
    "title": "Crime Hotspot Analysis",
    "section": "",
    "text": "Let’s import all the necessary libraries.\n\n\nCode\n# Basics\nimport pandas as pd\nimport geopandas as gpd\nfrom sklearn.cluster import dbscan\nfrom shapely.geometry import Point, MultiPolygon, Polygon\nimport time\n\n# Data Visualization-related Libraries\n\n## matplotlib\nimport matplotlib\n\n## folium\nimport folium\nimport panel as pn\n\npn.extension()\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nLoad in the pre-processed “LAcrime” data frame and the shapefile for all LAPD reporting districts.\n\n\nCode\nLAcrime = pd.read_parquet(\"LAcrime_trimmed\")\nLAPD_rd = gpd.read_file(\"./LAPD_Reporting_Districts_7103070480637650964/LAPD_Reporting_Districts.shp\")\n\n\nBefore performing the cluster analysis with DBSCAN, we first need to transform “LAcrime” into a GeoPandas data frame with a suitable geometry.\n\ngeometry = [Point(lon, lat) for lon, lat in zip(LAcrime['LON'], LAcrime['LAT'])]\nLAcrime = gpd.GeoDataFrame(LAcrime, geometry=geometry)\nLAcrime = LAcrime.set_crs(epsg = 4326, inplace = True)\nLAcrime = LAcrime.to_crs(epsg = LAPD_rd.crs.to_epsg())\nLAcrime[\"x\"] = LAcrime.geometry.x\nLAcrime[\"y\"] = LAcrime.geometry.y\n\nLAcrime.head()\n\n\n\n\n\n\n\n\nDATE OCC\nAREA\nCrm Cd\nCrm Cd Desc\nLAT\nLON\nDate\nYear\nMonth\nDay\nCrime_Description\nCrime Type\ngeometry\nx\ny\n\n\n\n\n4\n01/02/2010 12:00:00 AM\n1\n122\nRAPE, ATTEMPTED\n34.0387\n-118.2488\n2010-01-02\n2010\n1\nSaturday\nRape\nviolent\nPOINT (-13163396.203 4033999.675)\n-1.316340e+07\n4.034000e+06\n\n\n5\n01/04/2010 12:00:00 AM\n1\n442\nSHOPLIFTING - PETTY THEFT ($950 & UNDER)\n34.0480\n-118.2577\n2010-01-04\n2010\n1\nMonday\nOther Theft\nproperty\nPOINT (-13164386.946 4035249.076)\n-1.316439e+07\n4.035249e+06\n\n\n6\n01/07/2010 12:00:00 AM\n1\n330\nBURGLARY FROM VEHICLE\n34.0389\n-118.2643\n2010-01-07\n2010\n1\nThursday\nTheft from Vehicle\nproperty\nPOINT (-13165121.655 4034026.542)\n-1.316512e+07\n4.034027e+06\n\n\n7\n01/08/2010 12:00:00 AM\n1\n230\nASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT\n34.0435\n-118.2427\n2010-01-08\n2010\n1\nFriday\nAggravated Assaults\nviolent\nPOINT (-13162717.154 4034644.510)\n-1.316272e+07\n4.034645e+06\n\n\n8\n01/09/2010 12:00:00 AM\n1\n230\nASSAULT WITH DEADLY WEAPON, AGGRAVATED ASSAULT\n34.0450\n-118.2640\n2010-01-09\n2010\n1\nSaturday\nAggravated Assaults\nviolent\nPOINT (-13165088.259 4034846.028)\n-1.316509e+07\n4.034846e+06\n\n\n\n\n\n\n\nNow, setting a 1,000 m threshold and a minimum sample of 5, we perform the clustering with DBSCAN for each time period. This analysis aims at exploring the evolution of crime hotspots over time.\n\n\nCode\neps = 1000  # in meters\nmin_samples = 5\n\nstart_time = time.time()\n\nfor year in range((LAcrime[\"Year\"].unique())[0], (LAcrime[\"Year\"].unique())[-1] + 1):\n    for month in range((LAcrime[\"Month\"].unique())[0], (LAcrime[\"Month\"].unique())[-1] + 1):\n        temp = LAcrime[(LAcrime[\"Month\"] == month) & (LAcrime[\"Year\"] == year)]\n        cores1, labels1 = dbscan(temp[temp[\"Crime Type\"] == \"violent\"][[\"x\", \"y\"]],\n                                 eps = eps,\n                                 min_samples = min_samples)\n        cores2, labels2 = dbscan(temp[temp[\"Crime Type\"] == \"property\"][[\"x\", \"y\"]],\n                                 eps = eps,\n                                 min_samples = min_samples)\n        LAcrime.loc[(LAcrime[\"Crime Type\"] == \"violent\") & (LAcrime[\"Year\"] == year) & (LAcrime[\"Month\"] == month), \"label\"] = labels1\n        LAcrime.loc[(LAcrime[\"Crime Type\"] == \"property\") & (LAcrime[\"Year\"] == year) & (LAcrime[\"Month\"] == month), \"label\"] = labels2\n\nelapsed_time = time.time() - start_time\n\nprint(f\"Clustering completed in {elapsed_time:.2f} seconds.\")\n\n\nClustering completed in 42.07 seconds.\n\n\nOn top of showing the centroids of each cluster, we attempt to plot the convex hulls of each cluster as well.\n\n\nCode\n# Group by cluster label and generate convex hulls (or other boundaries) for each cluster.\ndef create_cluster_geometry(cluster_points):\n    return cluster_points.unary_union.convex_hull\n\n# Apply the geometry generation\nclustered_geometries = LAcrime[LAcrime[\"label\"] != -1].groupby([\"Year\", \"Month\", \"Crime Type\", \"label\"]).apply(lambda x: create_cluster_geometry(x.geometry))\n\n# Convert the resulting geometries into a MultiPolygon if there are multiple disjoint areas.\nmultipolygons = []\ncrime_labels = []\nYear = []\nMonth = []\n\nfor (year, month, crime_type, label), geom in clustered_geometries.items():\n    if isinstance(geom, Polygon):\n        multipolygons.append(geom)\n        crime_labels.append((crime_type, label))\n        Year.append(year)\n        Month.append(month)\n    elif isinstance(geom, MultiPolygon):\n        multipolygons.extend(list(geom))\n        crime_labels.extend([(crime_type, label)] * len(list(geom)))\n        Year.append(year)\n        Month.append(month)\n\n# Create a new GeoDataFrame to store the clusters' multipolygon geometries.\nLAcrime_clustered = gpd.GeoDataFrame({\"geometry\": multipolygons, \"Crime Type Label\": crime_labels, \"Year\": Year, \"Month\": Month}, crs = LAcrime.crs)\nLAcrime_clustered = LAcrime_clustered.to_crs(epsg = 4326)\n\n\nSet up the features for the base map consisting of LAPD reporting districts.\n\n\nCode\n# Generate 21 unique colors using a colormap.\nnum_precs = 21\ncmap = matplotlib.colormaps.get_cmap(\"tab20b\")\nprec_values = LAPD_rd[\"PREC\"].unique()\nprec_colors = {\n    prec: cmap(i / num_precs) for i, prec in enumerate(sorted(prec_values))\n}\n\n# Convert RGBA to Hex for folium compatibility.\nprec_colors_hex = {\n    prec: f'#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}' for prec, (r, g, b, _) in prec_colors.items()\n}\n\n# Define the style function.\ndef style_function(feature):\n    prec = feature[\"properties\"][\"PREC\"]\n    color = prec_colors_hex.get(prec, \"#808080\")\n    return {\n        \"fillColor\": color,\n        \"color\": \"black\",\n        \"weight\": 1,\n        \"fillOpacity\": 0.6,\n    }\n\n\nFor each time period, a hotspot map is returned. The red transparent polygons represent the convex hulls of the violent crimes clusters, and the blue polygons represent the convex hulls of the property crimes clusters. The centroids of violent and property crimes clusters are represented by red and green dots respectively. Noises for violent and property crimes are represented by grey and blue dots respectively.\n\n\nCode\ndef hotspot_map(year, month):\n    temp = LAcrime[(LAcrime[\"Month\"] == month) & (LAcrime[\"Year\"] == year)]\n    \n    # Initialize a map\n    m = folium.Map(location = [34.0522, -118.2437], zoom_start = 9)\n    \n    # Add LAPD reporting districts.\n    folium.GeoJson(\n        LAPD_rd,\n        style_function = style_function,\n        tooltip = folium.GeoJsonTooltip(fields = [\"PREC\"], aliases = [\"Precinct:\"], localize = True),\n        name = \"LAPD Reporting Districts\"\n    ).add_to(m)\n    \n    # Add cluster polygons for each crime type.\n    for idx, row in LAcrime_clustered[(LAcrime_clustered[\"Year\"] == year) & (LAcrime_clustered[\"Month\"] == month)].iterrows():\n        # Access the tuple components explicitly.\n        crime_type, label = row[\"Crime Type Label\"]\n\n        # Assign colors based on the crime type.\n        color = \"red\" if crime_type == \"violent\" else \"blue\"\n\n        # Create a descriptive name for each cluster layer.\n        layer_name = f\"{crime_type.capitalize()} Cluster {label}\"\n\n        # Add the geometry to the Folium map.\n        folium.GeoJson(\n            row[\"geometry\"],\n            style_function = lambda feature, color = color: {\n                \"fillColor\": color,\n                \"color\": color,\n                \"weight\": 1,\n                \"fillOpacity\": 0.25,\n            },\n            tooltip = f\"Crime Type: {crime_type}, Cluster: {label}\",\n            name = layer_name\n        ).add_to(m)\n        \n    # Step 4: Plot noise points and centroids for clusters.\n    for crime_type, color, marker_color in [(\"violent\", \"grey\", \"red\"), (\"property\", \"blue\", \"green\")]:\n        crime_data = temp[temp[\"Crime Type\"] == crime_type]\n\n        # Plot noise points.\n        noise_points = crime_data[crime_data[\"label\"] == -1]\n        for _, row in noise_points.iterrows():\n            folium.CircleMarker(\n                [row[\"LAT\"], row[\"LON\"]],\n                radius = 2,\n                color = color,\n                fill = True,\n                fill_color = color,\n                tooltip = f'Noise Point: {crime_type}',\n                name = f\"{crime_type.capitalize()} Noise Points\"\n            ).add_to(m)\n\n        # Plot centroids for each cluster.\n        for label in crime_data[\"label\"].unique():\n            if label == -1:\n                continue\n            cluster_data = crime_data[crime_data[\"label\"] == label]\n            centroid_x = cluster_data[\"LON\"].mean()\n            centroid_y = cluster_data[\"LAT\"].mean()\n            folium.CircleMarker(\n                [centroid_y, centroid_x],\n                radius = 2,\n                color = marker_color,\n                fill = True,\n                fill_color = marker_color,\n                tooltip = f\"{crime_type} Cluster {label} Centroid\",\n                name = f\"{crime_type.capitalize()} Cluster {label} Centroid\"\n            ).add_to(m)\n        \n    # Add Layer Control to toggle layers.\n    folium.LayerControl().add_to(m)\n    \n    # Return the map.\n    return pn.panel(m, height = 500, width = 800)\n\n\nAs a quick example, let’s look at the hotspot maps for January 2018, 2019, and 2020.\n\nhotspot_map(2018, 1)\n\n\n\n\n\n  \n\n\n\n\n\nhotspot_map(2019, 1)\n\n\n\n\n\n  \n\n\n\n\n\nhotspot_map(2020, 1)\n\n\n\n\n\n  \n\n\n\n\nWe observe that there are several persistent violent crime hotspots over the years: - One at the Northwest, covering precincts 9, 10, 15, 16, 17, 19, 21. - One in the middle, covering precincts 1, 2, 3, 4, 6, 7, 11, 12, 13. - Two smaller hotspots at the South, covering precinct 5. In particular, the violent crime hotspot in the middle is consistently large, and the two violent crime hotspots in precinct 5 are consistently small. On the other hand, The violent crime hotspot at the Northwest is more volatile in terms of size; in some periods it may breakdown into several smaller pieces.\nProperty crime hotspots, on the other hand, are more volatile. While we can identify some roughly persistent areas, the sizes of these hotspots vary time to time. But roughly speaking, there is a major property crime hotspot spanning the entire middle precincts, and sometimes even encompasses the Northwest precincts as well. Another one can be identified at the South.\nThese observations are general throughout the period from 2018 onwards. However, it warrants more caution when it comes to the hotspot maps for 2024, due to potential data incompleteness.\n\nRemark:\nSame as the crime distribution maps, below is an attempt to create a panel dashboard with year and month sliders for more interactive display. While it can be executed normally in Jupyter notebook, it needs to be hosted on dynamic servers. Again, this is part of the future work.\n\n\nCode\n# Create the sliders for year and month\nyear_selector = pn.widgets.IntSlider(name = \"Year\", start = 2010, end = 2024, value = 2018)\nmonth_selector = pn.widgets.IntSlider(name = \"Month\", start = 1, end = 12, value = 1)\n\n# Bind the function to the widgets\ninteractive_hotspot_map = pn.bind(hotspot_map, year = year_selector, month = month_selector)\n\n# Layout the widgets and map together\npn.Column(year_selector, month_selector, interactive_hotspot_map)",
    "crumbs": [
      "Analysis",
      "Crime Hotspot Analysis"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Discussion and Future Work",
    "section": "",
    "text": "This is the first attempt to perform clustering analysis to study the evolution of crime hotspots, and forecasting crimes using SARIMA models by areas. There are several lines of improvements:\n\n\nClustering analysis\n\nDifferent “eps” and “min_samples” may be more appropriate for different areas, rather than using the same set of parameters across all areas. For instance, it may be more appropriate to use different sets of parameters for urban and rural areas.\nLeveraging contextual information may provide insights on the parameter choices.\nDrawing convex hulls to represent the coverage of crime hotspots may cover more areas than what the data points suggest.\n\nForecasting\n\nSARIMA models are linear models. Non-linear models or approaches, such as machine learning models or neural networks, worth exploring.\nExplore whether there are any exogenous variables that might influence crime rates are available and measured monthly.",
    "crumbs": [
      "Discussion and Future Work"
    ]
  }
]